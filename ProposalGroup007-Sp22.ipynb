{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Chao-Li Wei (Michael)\n",
    "- Andrew Truong\n",
    "- Zeyu Feng (Ted)\n",
    "- Ahmad Said\n",
    "- Chiadika Vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Simultaneous tracking of multiple objects in a real-world environment has been active in the research field, even more so with the emerging popularity in the field of autonomous vehicles. The goal of our project is to perform Multiple Object Tracking (MOT) for the downsampled and subsetted BDD 100K dataset, containing over 2000 videos with 8 categories. Then, on a frame-by-frame basis, the model will predict bounding boxes to capture the objects present in the image and classify the corresponding objects. The performance will be measured by a handful of different evaluation metrics including percent accuracy and precision across all 8 categories, checking for false positives and negatives, as well as its F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Object tracking has been omnipotent in a variety of applications such as gesture recognizers, face identifiers, pose estimators<a name=\"pose\"></a>[<sup>[1]</sup>](#posenote), and scene analysis tools. However, for certain applications such as autonomous driving, the importance of accuracy of the model becomes especially important as tiny errors from the model can result in severe humanitarian casualties. Thus, the questions become what defines a good metric to evaluate the accuracy of the model, and what approach is plausible to push its performance.\n",
    "\n",
    "As there has not been agreement on a set of commonly applicable metrics for MOT, we will optimize the primary metrics multiple object tracking precision (MOTP) and the multiple object tracking accuracy (MOTA) proposed by Barnardin, which allow for objective comparison of the main characteristics of tracking systems<a name=\"mota\"></a>[<sup>[2]</sup>](#motanote). We will also use other supplementary metrics to provide a more holistic evaluation of the models.\n",
    "\n",
    "Finally, some existing methods to perform MOT use appearance features to associate objects across different frames, for example, SiameseCNN learns the similarity between a pair of detected objects with a Siamese Network <a name=\"siamese\"></a>[<sup>[9]</sup>](#siamesenote). For this project, we will follow the publication “DEFT: Detection Embeddings for Tracking” and implement a CenterNet as the object detection module and a Deep Affinity Network as the object matching module <a name=\"deft\"></a>[<sup>[6]</sup>](#deftnote). More details are provided in the [Proposed Solution](#proposed_solution) section below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem that we are trying to solve is multiple object tracking (MOT). Multiple object tracking can be summarized as the task of “locating multiple objects, maintaining their identities, and yielding their individual trajectories given an input video.”<a name=\"mot\"></a>[<sup>[3]</sup>](#motnote) For the BBD 100K dataset, our goal is very similar to the classic MOT task. Our goal is to predict 2D bounding boxes for objects and their association across frames in a video sequence of camera images. The camera images are frames from a video that show the view of an autonomous vehicle. The objects that we will be predicting are objects that the autonomous vehicle should detect while driving. The accuracy of our predictions will be based on many evaluation metrics like multiple object tracking accuracy (MOTA) and multiple object tracking precision (MOTP). These will be explained more in depth in our evaluation metrics section, but here’s a quick summary of each one. MOTA measures the errors from false positives, false negatives, and identity switches. On the other hand, MOTP measures the misalignments between ground-truths and detections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "To download the dataset, go to https://bdd-data.berkeley.edu/ , register for a free account and download the zip files. More instructions on parsing and installing the dataset can be found in the README file provided by BDD100K at https://github.com/SysCV/bdd100k-models/blob/main/doc/PREPARE_DATASET.md.\n",
    "\n",
    "\n",
    "Since the dataset provided by BDD100K is extremely large, we will be working on a subset of the training data that contains around 200 videos. Each video is around 40 seconds long and they are annotated at 5 fps (resampled from 30 fps), resulting in approximately 200 frames/images per video and a total of around 40000 frames/images in the training set. And all the images have 1280x720 pixels. We can convert each frame/image into a 3-dimensional array(x coordinates, y coordinates and RGB values) of numbers that has a shape of 1280x720x3 using the OpenCV package in Python.\n",
    "\n",
    "The annotations of each image contain information for the objects that should be detected and tracked at each frame. The labels for each object includes:\n",
    "- `id`: Each object has an unique id, and the same object across different frames keeps the same id. \n",
    "This can help us track the same object across different frames\n",
    "- `Category`: type of the object, “car” or “other vehicles”\n",
    "- `box2d`: Region Of Interest(bounding boxes) of the object and their positions on the frame indicated by x1, x2, y1, and y2 values. This is another important variable that can help us track the location of the object in the image/frame\n",
    "- `Attributes`\n",
    "    - `Occluded`: whether the object is occluded\n",
    "    - `Truncated`: whether the object is truncated\n",
    "\n",
    "The data is already resampled from 5 Hz to 30 Hz by BDD100K. The training data is likely going to be done on a subset of the original dataset, which takes about 56 GB of storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution \n",
    "<a id='proposed_solution'></a>\n",
    "We will implement the model in the publication “DEFT: Detection Embeddings for Tracking” that achieves a SOTA performance in Multiple Object Tracking<a name=\"deft\"></a>[<sup>[4]</sup>](#deftnote). \n",
    "\n",
    "Multiple Object Tracking can be splitted into 2 subtasks: Object Detection and Object Matching. First we need to identify or detect the objects in each frame, and then we need to match the same object across different frames to track the motion of that object. DEFT proposes a framework that has an object detection network jointly-learned with an object matching network.\n",
    "1. Use CenterNet <a name=\"centernet\"></a>[<sup>[5]</sup>](#centernetnote) as the backbone of the object detector that takes in a frame/image and outputs a set of bounding boxes that each box contains one unique object detected from the frame/image.\n",
    "2. Then we can extract feature embeddings from the objects detected in step one with convolutional layers.\n",
    "3. Next we can apply a Deep Affinity Network <a name=\"affinity\"></a>[<sup>[6]</sup>](#affinitynote) that uses the object feature embeddings from the previous step to calculate similarity scores between each pair of objects in both directions(forward:from frame t-1 to frame t; backward: from t-1 to from t). And we will create 2 matrices: A_fwd that contains similarity scores between objects in the forward direction and A_bwd that contains similarity scores between objects in the backward direction.\n",
    "4. Furthermore, DEFT defines a track T that contains the associated or matched detection of an object across frames. If the similarity score between an object and a track is greater than some threshold, then that object belongs to that track. If the object has a similarity score lower than the threshold with any of the existing tracks, that means the object is new or just entered the scene, we will create a new track to store the matched association of this new object. \n",
    "5. DEFT also utilizes an LSTM module <a name=\"lstm\"></a>[<sup>[7]</sup>](#lstmnote) for motion forecasting to filter out implausible trajectories. The LSTM module predicts the location of an object given its track of previous frames. Objects that are very far away from the predicted location will be set to have a negative similarity score with that track to remove physically impossible trajectories. \n",
    "6. For training, we will also create 2 ground truth Matching matrices. M_fwd, contains object matching information in the forward directions. M_fwd[i,j] is 1 if the ith object in the frame t-n is associated with the jth object in frame t, and 0 otherwise. M_bwd contains matching information in the backward directions.\n",
    "7. Loss function:(N_t is the number of objects detected in frame t and N_{t-n} is the number of objects detected in frame t-n)\n",
    "$$ L_{fwd} = \\sum_i\\sum_jM_{fwd}[i, j]log(A_{fwd}[i, j])$$\n",
    "$$ L_{bwd} = \\sum_i\\sum_jM_{bwd}[i, j]log(A_{bwd}[i, j])$$\n",
    "$$ L = \\frac{L_{fwd} + L_{bwd}}{2(N_t + N_{t-n})}$$\n",
    "\n",
    "Our solution will be tested and evaluated using the metrics in the [Evaluation Metrics](#evaluation_metrics) Section. It will also be compared to the benchmark model ResNet-50 that also utilizes similarity learning <a name=\"quasi\"></a>[<sup>[8]</sup>](#quasinote). Our plan is to first reproduce the ResNet-50 benchmark model's solution. Then, we'll build our model using PyTorch-Lightning and provide ample documentations to make our code reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "<a id='evaluation_metrics'></a>\n",
    "For this project, we will be focusing primarily on 5 evaluation metrics for our performance. The first would be multiple object tracking accuracy which would be given by the formula below\n",
    "$$MOTA = 1 - \\frac{\\sum_t(m_t+fp_t+mme_t)}{\\sum_tg_t}$$\n",
    "where $m_t$, $fp_t$, and $mme_t$ are the number of misses, false positives, and of mismatches respectively for time t, while $g_t$ is the total number of objects present in all frames. The second would be multiple object tracking precision, given by the equation below\n",
    "$$MOTP = \\frac{\\sum_{i, t}d_{i, t}}{\\sum_{t}c_{t}}$$\n",
    "where $d_t$ is the distance between the localization of objects in the ground truth and the detection output and $c_t$ is the total matches made between ground truth and the detection output. We will also be checking for the amount of false positives and false negatives we obtain as well as calculating our F1 score which combines the precision and recall of a classifier into a single metric by taking their harmonic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ethics and privacy, we are using a dataset of drivable environments that an autonomous vehicle would see, in it we will be encountering other cars, pedestrians, street signs, bikes and buildings. One privacy concern is that it will obviously have the faces and appearances of people around where the driving data was taken, but we are not specifically doing facial recognition and we will not make out who or what the person looks like crossing the road. All we’ll be doing is seeing if it is a person or not.\n",
    "Considering we keep our code hidden and only have a trained model, people will also not be able to identify pedestrians’ faces unless they themselves go and do it but considering how hard it is to identify faces from our dataset, we do not think this is much of a problem.\n",
    "\n",
    "Also ethically, autonomous vehicles could use our labeling of pedestrians to go out of its way to hit whoever it identifies as a pedestrian but it is vital to label pedestrians and it is important for an autonomous vehicle to not hit them we must label pedestrians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of us will be working on training the model, and will probably all be trying a unique solution out so work will be divided pretty evenly. \n",
    "\n",
    "* Expectation 1: We will meet weekly at the same time at 2:30 on Tuesdays to go over what we said we would do during the week.\n",
    "* Expectation 2: All work will be split pretty evenly and with agreement from the whole group so that one person is not shouldering all the load.\n",
    "* Expectation 3: Communication if an issue arises through the group discord. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 1/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 1/26  |  10 AM |  Do background research on topic (Pelé) | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 2/1  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets (Beckenbaur)  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/14  | 6 PM  | Import & Wrangle Data ,do some EDA (Maradonna) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 2/23  | 12 PM  | Finalize wrangling/EDA; Begin programming for project (Cruyff) | Discuss/edit project code; Complete project |\n",
    "| 3/13  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Carlos)| Discuss/edit full project |\n",
    "| 3/19  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"posenote\"></a>1.[^](#pose): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"posenote\"></a>1.[^](#pose): M. Voit, K. Nickel, and R. Stiefelhagen, “Multi-view head pose estimation using neural networks,” in Proceedings of the 2nd Workshop on Face Processing in Video (FPiV ’05), in association with the 2nd IEEE Canadian Conference on Computer and Robot Vision (CRV ’05), pp. 347–352, Victoria, Canada, May 2005. https://doi.org/10.1007/978-3-540-69568-4_26 <br>\n",
    "<a name=\"motanote\"></a>2.[^](#mota): Bernardin. (2008) Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics. https://link.springer.com/article/10.1155/2008/246309 <br>\n",
    "<a name=\"motnote\"></a>3.[^](#mot):Luo, Wenhan, et al. \"Multiple object tracking: A literature review.\" Artificial Intelligence 293 (2021): 103448. https://arxiv.org/abs/1409.7618<br>\n",
    "<a name=\"deftnote\"></a>4.[^](#deft): Chaabane, Mohamed, et al. \"Deft: Detection embeddings for tracking.\" arXiv preprint arXiv:2102.02267 (2021). https://arxiv.org/abs/2102.02267<br>\n",
    "<a name=\"centernetnote\"></a>5.[^](#centernet): Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. \"Objects as points.\" arXiv preprint arXiv:1904.07850 (2019). https://arxiv.org/abs/1904.07850<br>\n",
    "<a name=\"affinitynote\"></a>6.[^](#affinity): Sun, ShiJie, et al. \"Deep affinity network for multiple object tracking.\" IEEE transactions on pattern analysis and machine intelligence 43.1 (2019): 104-119. https://arxiv.org/abs/1810.11780<br>\n",
    "<a name=\"lstmnote\"></a>7.[^](#lstm): Sadeghian, Amir, Alexandre Alahi, and Silvio Savarese. \"Tracking the untrackable: Learning to track multiple cues with long-term dependencies.\" Proceedings of the IEEE international conference on computer vision. 2017. https://arxiv.org/abs/1701.01909<br>\n",
    "<a name=\"quasinote\"></a>8.[^](#quasi): Pang, Jiangmiao, et al. \"Quasi-dense similarity learning for multiple object tracking.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. https://arxiv.org/abs/2006.06664<br>\n",
    "<a name=\"siamesenote\"></a>9.[^](#siamese): He, Anfeng, et al. \"A twofold siamese network for real-time object tracking.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. https://arxiv.org/abs/1802.08817v1<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
