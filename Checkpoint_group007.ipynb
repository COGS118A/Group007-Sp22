{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Chao-Li Wei (Michael)\n",
    "- Andrew Truong\n",
    "- Zeyu Feng (Ted)\n",
    "- Ahmad Said\n",
    "- Chiadika Vincent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Simultaneous tracking of multiple objects in a real-world environment has been active in the research field, even more so with the emerging popularity in the field of autonomous vehicles. The goal of our project is to perform Multiple Object Tracking (MOT) for the downsampled and subsetted BDD 100K dataset, containing over 2000 videos with 8 categories. Then, on a frame-by-frame basis, the model will predict bounding boxes to capture the objects present in the image and classify the corresponding objects. The performance will be measured by a handful of different evaluation metrics including percent accuracy and precision across all 8 categories, checking for false positives and negatives, as well as its F1 score. The main contribution of our work will be performing hyperparameter tuning on the baseline model by the competition, creating visualization to interpret the model, and performing analysis on the evaluation metrics to motivate future directions of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Object tracking has been omnipotent in a variety of applications such as gesture recognizers, face identifiers, pose estimators<a name=\"pose\"></a>[<sup>[1]</sup>](#posenote), and scene analysis tools. However, for certain applications such as autonomous driving, the importance of accuracy of the model becomes especially important as tiny errors from the model can result in severe humanitarian casualties. Thus, the questions become what defines a good metric to evaluate the accuracy of the model, and what approach is plausible to push its performance.\n",
    "\n",
    "As there has not been agreement on a set of commonly applicable metrics for MOT, we will optimize the primary metrics multiple object tracking precision (MOTP) and the multiple object tracking accuracy (MOTA) proposed by Barnardin, which allow for objective comparison of the main characteristics of tracking systems<a name=\"mota\"></a>[<sup>[2]</sup>](#motanote). We will also use other supplementary metrics to provide a more holistic evaluation of the models.\n",
    "\n",
    "In addition, some existing methods to perform MOT use appearance features to associate objects across different frames, for example, SiameseCNN learns the similarity between a pair of detected objects with a Siamese Network <a name=\"siamese\"></a>[<sup>[9]</sup>](#siamesenote). In “DEFT: Detection Embeddings for Tracking”, the researchers implement a CenterNet as the object detection module and a Deep Affinity Network as the object matching module <a name=\"deft\"></a>[<sup>[6]</sup>](#deftnote). More details are provided in the [Proposed Solution](#proposed_solution) section below. \n",
    "\n",
    "For our work, we will push the performance of the baseline model FRCNN ... [Ted provide citation and describe the method briefly](#delete) with hyperparameter tuning. Furthermore, by performing an analysis on the various metrics and interpreting the model layers, we hope to eventually motivate future directions for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The problem that we are trying to solve is multiple object tracking (MOT). Multiple object tracking can be summarized as the task of “locating multiple objects, maintaining their identities, and yielding their individual trajectories given an input video.”<a name=\"mot\"></a>[<sup>[3]</sup>](#motnote) For the BBD 100K dataset, our goal is very similar to the classic MOT task. Our goal is to predict 2D bounding boxes for objects and their association across frames in a video sequence of camera images. The camera images are frames from a video that show the view of an autonomous vehicle. The objects that we will be predicting are objects that the autonomous vehicle should detect while driving. The accuracy of our predictions will be based on many evaluation metrics like multiple object tracking accuracy (MOTA) and multiple object tracking precision (MOTP). These will be explained more in depth in our evaluation metrics section, but here’s a quick summary of each one. MOTA measures the errors from false positives, false negatives, and identity switches. On the other hand, MOTP measures the misalignments between ground-truths and detections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "To download the dataset, go to https://bdd-data.berkeley.edu/ , register for a free account and download the zip files. More instructions on parsing and installing the dataset can be found in the README file provided by BDD100K at https://github.com/SysCV/bdd100k-models/blob/main/doc/PREPARE_DATASET.md.\n",
    "\n",
    "\n",
    "Since the dataset provided by BDD100K is extremely large, we will be working on a subset of the training data that contains around 200 videos. Each video is around 40 seconds long and they are annotated at 5 fps (resampled from 30 fps), resulting in approximately 200 frames/images per video and a total of around 40000 frames/images in the training set. And all the images have 1280x720 pixels. We can convert each frame/image into a 3-dimensional array(x coordinates, y coordinates and RGB values) of numbers that has a shape of 1280x720x3 using the OpenCV package in Python.\n",
    "\n",
    "The annotations of each image contain information for the objects that should be detected and tracked at each frame. The labels for each object includes:\n",
    "- `id`: Each object has an unique id, and the same object across different frames keeps the same id. \n",
    "This can help us track the same object across different frames\n",
    "- `Category`: type of the object, “car” or “other vehicles”\n",
    "- `box2d`: Region Of Interest(bounding boxes) of the object and their positions on the frame indicated by x1, x2, y1, and y2 values. This is another important variable that can help us track the location of the object in the image/frame\n",
    "- `Attributes`\n",
    "    - `Occluded`: whether the object is occluded\n",
    "    - `Truncated`: whether the object is truncated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Size Discussion\n",
    "The data is already resampled from 5 Hz to 30 Hz by BDD100K. Furthermore, the training data is likely going to be done on a subset of the original dataset, which takes about 56 GB of storage. Currently the subset of the data we use is still about 5.6 GB. Given the size of the baseline model, running a single epoch of the data take about 4 days on a 20GB RAM CPU with Intel Core i7 processor. On the other hand, running sinlge epoch takes about 19 minutes using 2 GeForce RTX 208 GPUs. Therefore, using a GPU is strongly recommended to reproduce the results. For setting up compatible CUDA Driver and PyTorch+CUDA version, refer to [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.\n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model[3] against which your solution will be compared.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='proposed_solution'></a>\n",
    "Please describe the baseline model approach instead [@Ted](#delete) <br>\n",
    "We can first finetune hyper-parameters of the baseline model QDTrack which adopts a Quasi-Dense Similarity learning methods to find similar objects in different frames and uses tracks to keep track of the same object across different frames(the same object should be in the same track across different frames). And the hyper-parameters we can finetune are:\n",
    "   1. match_score_thr(0-1.0): The threshold to associate objects across different frames\n",
    "   2. init_score_thr(0-1.0): The score threshold to starts a new track\n",
    "   3. obj_score_thr(0-1.0): The score threshold to continue a track or the detected object is from a previous track\n",
    "   4. memo_tracklet_frames(natural number): The number of frames to keep tracks or the number of frames we should keep the tracks even if the object is not in the frame\n",
    "   5. memo_backdrop_frames(natural number): The number of frames to keep backdrops which are helpful to reduce false positive when matching obejctes in different frames\n",
    "   6. memo_momentum(0-1.0): The momentum to update the embeddings\n",
    "   7. match_metric: The Matching metric \n",
    "\n",
    "We will implement the model in the publication “DEFT: Detection Embeddings for Tracking” that achieves a SOTA performance in Multiple Object Tracking<a name=\"deft\"></a>[<sup>[4]</sup>](#deftnote). \n",
    "Multiple Object Tracking can be splitted into 2 subtasks: Object Detection and Object Matching. First we need to identify or detect the objects in each frame, and then we need to match the same object across different frames to track the motion of that object. DEFT proposes a framework that has an object detection network jointly-learned with an object matching network.\n",
    "1. Use CenterNet <a name=\"centernet\"></a>[<sup>[5]</sup>](#centernetnote) as the backbone of the object detector that takes in a frame/image and outputs a set of bounding boxes that each box contains one unique object detected from the frame/image.\n",
    "2. Then we can extract feature embeddings from the objects detected in step one with convolutional layers.\n",
    "3. Next we can apply a Deep Affinity Network <a name=\"affinity\"></a>[<sup>[6]</sup>](#affinitynote) that uses the object feature embeddings from the previous step to calculate similarity scores between each pair of objects in both directions(forward:from frame t-1 to frame t; backward: from t-1 to from t). And we will create 2 matrices: A_fwd that contains similarity scores between objects in the forward direction and A_bwd that contains similarity scores between objects in the backward direction.\n",
    "4. Furthermore, DEFT defines a track T that contains the associated or matched detection of an object across frames. If the similarity score between an object and a track is greater than some threshold, then that object belongs to that track. If the object has a similarity score lower than the threshold with any of the existing tracks, that means the object is new or just entered the scene, we will create a new track to store the matched association of this new object. \n",
    "5. DEFT also utilizes an LSTM module <a name=\"lstm\"></a>[<sup>[7]</sup>](#lstmnote) for motion forecasting to filter out implausible trajectories. The LSTM module predicts the location of an object given its track of previous frames. Objects that are very far away from the predicted location will be set to have a negative similarity score with that track to remove physically impossible trajectories. \n",
    "6. For training, we will also create 2 ground truth Matching matrices. M_fwd, contains object matching information in the forward directions. M_fwd[i,j] is 1 if the ith object in the frame t-n is associated with the jth object in frame t, and 0 otherwise. M_bwd contains matching information in the backward directions.\n",
    "7. Loss function:(N_t is the number of objects detected in frame t and N_{t-n} is the number of objects detected in frame t-n)\n",
    "$$ L_{fwd} = \\sum_i\\sum_jM_{fwd}[i, j]log(A_{fwd}[i, j])$$\n",
    "$$ L_{bwd} = \\sum_i\\sum_jM_{bwd}[i, j]log(A_{bwd}[i, j])$$\n",
    "$$ L = \\frac{L_{fwd} + L_{bwd}}{2(N_t + N_{t-n})}$$\n",
    "\n",
    "Our solution will be tested and evaluated using the metrics in the [Evaluation Metrics](#evaluation_metrics) Section. It will also be compared to the benchmark model ResNet-50 that also utilizes similarity learning <a name=\"quasi\"></a>[<sup>[8]</sup>](#quasinote). Our plan is to first reproduce the ResNet-50 benchmark model's solution. Then, we'll build our model using PyTorch-Lightning and provide ample documentations to make our code reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "<a id='evaluation_metrics'></a>\n",
    "For this project, we will be focusing primarily on 5 evaluation metrics for our performance. The first would be multiple object tracking accuracy which would be given by the formula below\n",
    "$$MOTA = 1 - \\frac{\\sum_t(m_t+fp_t+mme_t)}{\\sum_tg_t}$$\n",
    "where $m_t$, $fp_t$, and $mme_t$ are the number of misses, false positives, and of mismatches respectively for time t, while $g_t$ is the total number of objects present in all frames. The second would be multiple object tracking precision, given by the equation below\n",
    "$$MOTP = \\frac{\\sum_{i, t}d_{i, t}}{\\sum_{t}c_{t}}$$\n",
    "where $d_t$ is the distance between the localization of objects in the ground truth and the detection output and $c_t$ is the total matches made between ground truth and the detection output. We will also be checking for the amount of false positives and false negatives we obtain as well as calculating our F1 score which combines the precision and recall of a classifier into a single metric by taking their harmonic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "Please show any preliminary results you have managed to obtain.\n",
    "\n",
    "Examples would include:\n",
    "- Analyzing the suitability of a dataset or alogrithm for prediction/solving your problem \n",
    "- Performing feature selection or hand-designing features from the raw data. Describe the features available/created and/or show the code for selection/creation\n",
    "- Showing the performance of a base model/hyper-parameter setting.  Solve the task with one \"default\" algorithm and characterize the performance level of that base model.\n",
    "- Learning curves or validation curves for a particular model\n",
    "- Tables/graphs showing the performance of different models/hyper-parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Results for BBOX (Object Detection)\n",
    "![title](reports/figures/baseline/bbox_eval_output.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Results for Track (Tracking)\n",
    "![title](reports/figures/baseline/track_eval_output.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the next checkpoint\n",
    "* The learning curve will be plotted\n",
    "* The baseline model will be compared with the model trained by our group\n",
    "\n",
    "By the next 2 checkpoints\n",
    "* The baseline model will be compared with the hyperparameter-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ethics and privacy, we are using a dataset of drivable environments that an autonomous vehicle would see, in it we will be encountering other cars, pedestrians, street signs, bikes and buildings. One privacy concern is that it will obviously have the faces and appearances of people around where the driving data was taken, but we are not specifically doing facial recognition and we will not make out who or what the person looks like crossing the road. All we’ll be doing is seeing if it is a person or not.\n",
    "Considering we keep our code hidden and only have a trained model, people will also not be able to identify pedestrians’ faces unless they themselves go and do it but considering how hard it is to identify faces from our dataset, we do not think this is much of a problem.\n",
    "\n",
    "Also ethically, our solution could make mistakes and mislabel a pedestrian crossing a street as a lightpost or a stationary object or not even see it and accidently hit it. We can only address these issues by making sure that our model does not make many of these mistakes. But these are extremely rare and just our solution would not be used in guiding an autonomous vehicle. Autonomous vehicles usually come equipped with LIDAR/RADAR and have a pipeline of image processing to make sure it does not make mistakes in labeling people. So there will be many redundancies in case we mislabel a pedestrian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of us will be working on training the model, and will probably all be trying a unique solution out so work will be divided pretty evenly. \n",
    "\n",
    "* Expectation 1: We will meet weekly at the same time at 2:30 on Tuesdays to go over what we said we would do from last week's meeting. This meeting should last about an hour. \n",
    "* Expectation 2: All work will be split pretty evenly and with agreement from the whole group so that one person is not shouldering all the load. we will try to cater to each person's strengths and preferences.\n",
    "* Expectation 3: Communication if an issue arises through the group discord. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 4/26  |  2:30 PM |  Downloaded all the dataset from BDD100K website  |   | \n",
    "| 5/3  |  2:30 PM |  Followed all the steps provided by qdtrack to symlink the project structure | Next steps to be done to reproduce evaluation results | \n",
    "| 5/10  | 2:30 PM | Set up virtual environment such that all packages are compatible to run the training and evaluation code  |  Address any issues with compute constraints  |\n",
    "| 5/17  | 2:30 PM  | 3 out of 5 members have CUDA setup and is able to run the evaluation script provided by qdtrack   | Fix errors that come with training subset of data instead of the entire dataset\n",
    "| 5/24  | 2:30 PM  | Completed training the model and save the models in the respective folder   | Discuss the figures to be completed, analysis to be performed\n",
    "| 5/31  | 2:30 PM  | Finished hyperparameter search. Cached misclassified images for evaluation. Visualized layers of the model if time allowed.   | Discuss the remaining tasks to be done, make sure all checkpoints are met\n",
    "| 6/7   | 2:30 PM  | Completed individual parts (all) | Combine all our parts of code; Read through the entire report\n",
    "| 6/8  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"posenote\"></a>1.[^](#pose): M. Voit, K. Nickel, and R. Stiefelhagen, “Multi-view head pose estimation using neural networks,” in Proceedings of the 2nd Workshop on Face Processing in Video (FPiV ’05), in association with the 2nd IEEE Canadian Conference on Computer and Robot Vision (CRV ’05), pp. 347–352, Victoria, Canada, May 2005. https://doi.org/10.1007/978-3-540-69568-4_26 <br>\n",
    "<a name=\"motanote\"></a>2.[^](#mota): Bernardin. (2008) Evaluating Multiple Object Tracking Performance: The CLEAR MOT Metrics. https://link.springer.com/article/10.1155/2008/246309 <br>\n",
    "<a name=\"motnote\"></a>3.[^](#mot):Luo, Wenhan, et al. \"Multiple object tracking: A literature review.\" Artificial Intelligence 293 (2021): 103448. https://arxiv.org/abs/1409.7618<br>\n",
    "<a name=\"deftnote\"></a>4.[^](#deft): Chaabane, Mohamed, et al. \"Deft: Detection embeddings for tracking.\" arXiv preprint arXiv:2102.02267 (2021). https://arxiv.org/abs/2102.02267<br>\n",
    "<a name=\"centernetnote\"></a>5.[^](#centernet): Zhou, Xingyi, Dequan Wang, and Philipp Krähenbühl. \"Objects as points.\" arXiv preprint arXiv:1904.07850 (2019). https://arxiv.org/abs/1904.07850<br>\n",
    "<a name=\"affinitynote\"></a>6.[^](#affinity): Sun, ShiJie, et al. \"Deep affinity network for multiple object tracking.\" IEEE transactions on pattern analysis and machine intelligence 43.1 (2019): 104-119. https://arxiv.org/abs/1810.11780<br>\n",
    "<a name=\"lstmnote\"></a>7.[^](#lstm): Sadeghian, Amir, Alexandre Alahi, and Silvio Savarese. \"Tracking the untrackable: Learning to track multiple cues with long-term dependencies.\" Proceedings of the IEEE international conference on computer vision. 2017. https://arxiv.org/abs/1701.01909<br>\n",
    "<a name=\"quasinote\"></a>8.[^](#quasi): Pang, Jiangmiao, et al. \"Quasi-dense similarity learning for multiple object tracking.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. https://arxiv.org/abs/2006.06664<br>\n",
    "<a name=\"siamesenote\"></a>9.[^](#siamese): He, Anfeng, et al. \"A twofold siamese network for real-time object tracking.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. https://arxiv.org/abs/1802.08817v1<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA Driver, CUDA+PyTorch Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to make CUDA 11.3 work with mmcv-full/pytorch on a Windows machine:\n",
    "- Set up WSL2 on your machine first in powershell(may need admin powershell)\n",
    "`wsl --install`\n",
    "    - Make sure you have WSL2 installed and not WSL1\n",
    "`wsl -l -v.`\n",
    "- Go into the microsoft store and download Ubuntu 22.04\n",
    "- Install conda on Ubuntu 22.04\n",
    "- Create conda environment with python version 3.8\n",
    "`conda create -n qdtrack python=3.8 -y`\n",
    "- Install CUDA 11.3 via the WSL instructions\n",
    "    - Recommend doing this in /tmp/ or create a temp folder like `/install/` before running this\n",
    "\n",
    "- Install pytorch 1.11 that’s compatible with cuda 11.3\n",
    "`conda install pytorch torchvision torchaudio cudatoolkit=11.3 -c pytorch`\n",
    "- Test if cuda is available\n",
    "`python \n",
    "import torch\n",
    "print(torch.cuda.is_available())`\n",
    "and hopefully returns true\n",
    "\n",
    "\n",
    "- Install mmcv-full 1.4.7 that’s compatible with cuda 11.3\n",
    "`pip install mmcv-full==1.4.7 -f https://download.openmmlab.com/mmcv/dist/cu113/torch1.11.0/index.html`\n",
    "- Install the latest version of mmdet(using 2.24.1)\n",
    "`pip install mmdet`\n",
    "- Install qdtrack from the qdtrack git repository\n",
    "`python setup.py develop`\n",
    "- Install other packages needed from the requirements.txt just in case\n",
    "`pip install lvis motmetrics pycocotools seaborn`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Packages and Tools Downloading Instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download gcc on windows: https://dev.to/gamegods3/how-to-install-gcc-in-windows-10-the-easier-way-422j "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
